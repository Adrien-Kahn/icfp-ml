{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bdafe7b",
   "metadata": {},
   "source": [
    "# Your name:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a327c",
   "metadata": {},
   "source": [
    "# Gaussian mixture model: EM vs VI\n",
    "\n",
    "\n",
    "We consider two algorithms for inference in the Gaussian mixture model: expectation maximization (EM) and mean-field variational inference (VI).\n",
    "\n",
    "In both cases, we deal with a mixture of $K$ real-valued normalized Gaussian random variables. We assume that $K$ is known and for simplicity that the variances of the Gaussian distributions are all equal to one.There are $n$ observations denoted by $x=(x_1,\\dots, x_n)\\in \\mathbb{R}^n$. The weights of the mixture as well as the mean of each Gaussian distribution is not known and need to be estimated.\n",
    "\n",
    "Note: you can read Sections 1 and 2 independently (as a result some notations are repeated).\n",
    "\n",
    "## 1. Expectation Maximization\n",
    "\n",
    "**setting:**\n",
    "- $\\mu=(\\mu_1,\\dots, \\mu_K)\\in \\mathbb{R}^K$\n",
    "- $p(c=k) = \\pi_k$ with $\\pi =(\\pi_1,\\dots, \\pi_K)$ and $\\sum_k \\pi_k =1$, $\\pi_k\\geq 0$\n",
    "- parameter $\\theta=(\\mu,\\pi)$\n",
    "- Gaussian distribution with normalized variance and mean $m$: $f_m(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}(x-m)^2 \\right)$.\n",
    "- Gaussian mixture model: for $x=(x_1,\\dots, x_n)\\in \\mathbb{R}^n$ and $c = (c_1,\\dots c_n)\\in [K]^n$, we have $p_{\\theta}(x,c) = \\prod_{i=1}^n \\pi_{c_i} f_{\\mu_{c_i}}(x_i)$\n",
    "\n",
    "Likelihood (we observe only $x$ and not the classes given by $c$)\n",
    "\\begin{align*}\n",
    "\\ell(\\theta) &= \\ln p_\\theta(x) = \\ln \\sum_{c\\in [K]^n} p_\\theta(x,c)\n",
    "\\end{align*}\n",
    "is not tractable because the time complexity for evaluating the sum is $O(K^n)$.\n",
    "\n",
    "**ELBO**\n",
    "\\begin{align*}\n",
    "\\ell(\\theta) &= \\sum_{c\\in [K]^n} q(c) \\ln  p_\\theta(x)\\\\\n",
    "&= \\sum_{c\\in [K]^n} q(c) \\ln \\frac{p_\\theta(x,c)q(c)}{p_{\\theta}(c|x)q(c)}\\\\\n",
    "&= \\sum_{c\\in [K]^n} q(c) \\ln \\frac{p_\\theta(x,c)}{q(c)} + \\sum_{c\\in [K]^n} q(c) \\ln \\frac{q(c)}{p_{\\theta}(c|x)}\\\\ \n",
    "&= \\text{ELBO}(q)+\\text{KL}\\left(q(c)\\| p_\\theta(c|x)\\right)\n",
    "%&= \\sum_{c\\in [K]^n} q(c|x) \\ln p_\\theta(x,c) + H(q(c|x))\n",
    "%&\\geq \\sum_{c\\in [K]^n} q(c|x) \\ln p_\\theta(x,c) \n",
    "\\end{align*}\n",
    "\n",
    "Note that for $q(c) = p_{\\theta}(c|x)$, we get $\\text{KL}\\left(p_{\\theta}(c|x)\\| p_\\theta(c|x)\\right)=0$, so that $\\text{ELBO}(p_{\\theta}(c|x)) =\\ell(\\theta)$.\n",
    "Now for an estimate $\\theta_t$ of the parameter, we take $q(c) = p_{\\theta_t}(c|x)$ and define\n",
    "\\begin{align*}\n",
    "\\ell(\\theta|\\theta_t)\\stackrel{\\Delta}{=} \\sum_{c\\in [K]^n} p_{\\theta_t}(c|x) \\ln \\frac{p_\\theta(x,c)}{p_{\\theta_t}(c|x)}, %= \\text{ELBO}( p_{\\theta_t}(c|x)),\n",
    "\\end{align*}\n",
    "so that by previous argument, we have\n",
    "\\begin{align*}\n",
    "\\ell(\\theta|\\theta_t) = \\ell(\\theta)-\\text{KL}\\left(p_{\\theta_t}(c|x)\\| p_\\theta(c|x)\\right),\n",
    "\\end{align*}\n",
    "in particular, we see that $\\ell(\\theta|\\theta_t) \\leq  \\ell(\\theta)$ and $\\ell(\\theta_t|\\theta_t) = \\ell(\\theta_t)$.\n",
    "\n",
    "**EM algorithm:**\n",
    "- E-step: compute $p_{\\theta_t}(c|x)$\n",
    "- M-step: maximize in $\\theta$, $\\theta_{t+1}=\\arg\\max_{\\theta}\\ell(\\theta|\\theta_t)=\\arg\\max_{\\theta}\\sum_{c\\in [K]^n} p_{\\theta_t}(c|x) \\ln {p_\\theta(x,c)}$\n",
    "\n",
    "**Properties of EM:** the sequence of log-likelihoods $(\\ell(\\theta_t))_{t\\geq 0}$ is non-decreasing and thus converges.\n",
    "\n",
    "The proof is easy: note that by applying the equality above for $\\theta=\\theta_{t+1}$, we get\n",
    "\\begin{align*}\n",
    "\\ell(\\theta_{t+1}|\\theta_t) = \\ell(\\theta_{t+1})-\\text{KL}\\left(p_{\\theta_t}(c|x)\\| p_{\\theta_{t+1}}(c|x)\\right)\\leq \\ell(\\theta_{t+1}),\n",
    "\\end{align*}\n",
    "so that we have\n",
    "\\begin{align*}\n",
    "\\ell(\\theta_{t+1}) \\geq \\ell(\\theta_{t+1}|\\theta_t) \\geq \\ell(\\theta_{t}|\\theta_t) = \\ell(\\theta_t)\n",
    "\\end{align*}\n",
    "\n",
    "We now need to derive the EM algorithm for the GMM.\n",
    "\n",
    "**Question 1**\n",
    "Show that for a parameter $\\theta =(\\mu, \\pi)$ and $x\\in \\mathbb{R}^n$, we have:\n",
    "\\begin{align*}\n",
    "p_{ik}(\\theta,x) &\\stackrel{\\Delta}{=} p_{\\theta}(c_i=k|x_i)\\\\\n",
    "& = \\frac{\\pi_k(\\theta) \\exp\\left( -\\frac{1}{2}(x_i-\\mu_k(\\theta))^2\\right)}{\\sum_\\ell \\pi_\\ell(\\theta) \\exp\\left( -\\frac{1}{2}(x_i-\\mu_\\ell(\\theta))^2\\right)},\n",
    "\\end{align*}\n",
    "with a slight abuse of notation where $\\theta= (\\mu(\\theta),\\pi(\\theta))$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2476e9f",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "We define\n",
    "\\begin{align*}\n",
    "n_{k}(\\theta, x) = \\sum_{i=1}^n p_{ik}(\\theta,x).\n",
    "\\end{align*}\n",
    "\n",
    "Derive the **EM for Gaussian mixture**\n",
    "- **Expectation** given $\\theta_t$ and the samples $x=(x_1,\\dots, x_n)$, compute for all $i\\in [n]$ and $k\\in [K]$:\n",
    "\\begin{align*}\n",
    "p_{ik}(\\theta_t,x) & = ???\n",
    "\\end{align*}\n",
    "- **Maximization** given $\\theta_t$ and the samples $x=(x_1,\\dots, x_n)$, the parameter $\\theta_{t+1}$ maximizing $\\ell(\\theta|\\theta_t)$ is given by\n",
    "\\begin{align*}\n",
    "\\pi_k(\\theta_{t+1}) &= ???\\\\\n",
    "\\mu_k(\\theta_{t+1}) &= ???\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f1fdd7",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "Compute $\\ell(\\theta_t)=\\ell(\\theta_t|\\theta_t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2ff69d",
   "metadata": {},
   "source": [
    "## 2. Mean-field Variational Inference\n",
    "\n",
    "**setting:**\n",
    "- $\\mu=(\\mu_1,\\dots, \\mu_K)\\in \\mathbb{R}^K$ with prior $p(\\mu_k)=\\mathcal{N}(\\mu_k|0,\\sigma^2)$ and $p(\\mu)= \\prod_k p(\\mu_k)$.\n",
    "- $p(c=k) = \\pi_k$ with $\\pi =(\\pi_1,\\dots, \\pi_K)$ and $\\sum_k \\pi_k =1$, $\\pi_k\\geq 0$ with a Dirichlet prior $p(\\pi)=C(\\alpha)\\prod_{k=1}^K\\pi_k^{\\alpha-1}$, with $C(\\alpha) = \\frac{\\Gamma(K\\alpha)}{\\Gamma(\\alpha)^K}$.\n",
    "- Gaussian distribution with normalized variance and mean $m$: $f_m(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}(x-m)^2 \\right)$.\n",
    "- Gaussian mixture: for $x=(x_1,\\dots, x_n)\\in \\mathbb{R}^n$ and $c = (c_1,\\dots c_n)\\in [K]^n$, we have $p(x,c,\\mu,\\pi) = p(\\mu)p(\\pi)\\prod_{i=1}^n \\pi_{c_i} f_{\\mu_{c_i}}(x_i)$\n",
    "\n",
    "Hence, the evidence is\n",
    "\\begin{align*}\n",
    "p(x) = \\sum_{c\\in [K]^n} \\int_{\\mu,\\pi} p(\\mu)p(\\pi)\\prod_{i=1}^n \\pi_{c_i} f_{\\mu_{c_i}}(x_i) d\\mu d\\pi = \\int_{\\mu,\\pi} p(\\mu)p(\\pi)\\prod_{i=1}^n\\sum_{c_i\\in[K]} \\pi_{c_i} f_{\\mu_{c_i}}(x_i) d\\mu d\\pi\n",
    "\\end{align*}\n",
    "\n",
    "**ELBO**\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\ln p(x) &= \\sum_c\\int_{\\mu,\\pi} q(\\mu,\\pi,c)\\ln \\frac{p(x,c,\\mu,\\pi)}{p(c,\\mu,\\pi|x)} d\\mu d\\pi\\\\\n",
    "&= \\sum_c\\int_{\\mu,\\pi} q(\\mu,\\pi,c)\\ln \\frac{p(x,c,\\mu,\\pi)}{q(\\mu,\\pi,c)}-\\sum_c\\int_{\\mu,\\pi} q(\\mu, \\pi,c)\\ln \\frac{p(c,\\mu, \\pi|x)}{q(\\mu, \\pi,c)}\\\\\n",
    "&= \\text{ELBO}(q)+\\text{KL}\\left( q(\\mu,\\pi,c)\\| p(c,\\mu,\\pi|x)\\right)\n",
    "\\end{align*}\n",
    "\n",
    "**Maximizing ELBO with a mean-field distribution**\n",
    "\n",
    "We are considering now factorized distributions for $q$:\n",
    "\\begin{align*}\n",
    "q(\\mu,\\pi,c) = q(\\mu,\\pi)q(c).\n",
    "\\end{align*}\n",
    "\n",
    "We have for the factor $q(\\mu,\\pi)$:\n",
    "\\begin{align*}\n",
    "\\text{ELBO}(q) &= \\int_{\\mu, \\pi} q(\\mu,\\pi)\\sum_{c\\in [K]^n} q(c)\\ln p(x, c,\\mu,\\pi)d\\mu d\\pi +H(q(\\mu,\\pi))+H(q(c))\\\\\n",
    "&=\\int_{\\mu,\\pi} q(\\mu,\\pi)\\mathbb{E}_{q(c)}\\left[\\ln p(x, c,\\mu,\\pi)\\right]d\\mu d\\pi-\\int_{\\mu,\\pi} q(\\mu,\\pi)\\ln q(\\mu,\\pi)d\\mu d\\pi +\\text{cst}\n",
    "\\end{align*}\n",
    "\n",
    "**Question 4** \n",
    "When $q(c)$ is fixed (we denote $\\varphi_{i}(k) = q(c_i=k)$ the $i$-th marginal of $q$), find the distirbution $q^*(\\mu,\\pi)$ maximizing ELBO.\n",
    "(Hint: if you are not familiar with the Dirichlet distribution, have a look at its [definition](https://en.wikipedia.org/wiki/Dirichlet_distribution))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4b87b9",
   "metadata": {},
   "source": [
    "**Question 5**\n",
    "Similarly maximizing ELBO with $q(\\mu,\\pi)$ fixed, give the distribution $q^*(c)$. (Hint: the log moment of the [Dirichlet distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution) are given by the [digamma function](https://en.wikipedia.org/wiki/Digamma_function))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b657eaac",
   "metadata": {},
   "source": [
    "**Question 6** Derive the mean-fild VI algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1552127",
   "metadata": {},
   "source": [
    "**Question 7** To compute the ELBO,  the following decomposition\n",
    "\\begin{align*}\n",
    "\\text{ELBO}(q) = \\underbrace{\\mathbb{E}_{q(\\mu)q(\\pi)}\\ln p(\\mu)p(\\pi)}_{A}+\\underbrace{\\mathbb{E}_{q(\\mu)q(\\pi)q(c)}\\ln \\prod_{i=1}^n\\pi_{c_i}f_{\\mu_{c_i}}(x_i)}_{B} +H(q(\\mu))+H(q(\\pi))+H(q(c)).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e178c981",
   "metadata": {},
   "source": [
    "**Question 8** Condier the **Uniformative priors** by taking $\\sigma^2\\to\\infty$ and $\\alpha \\to 0$ and use the fact that as $z\\to\\infty$, we have $\\psi(z) \\sim \\ln(z)-\\frac{1}{2z}$ to show the similarity between EM and VI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c400cf0",
   "metadata": {},
   "source": [
    "# 3. Code\n",
    "\n",
    "We now code both algorithms.\n",
    "\n",
    "The code below, will produce data for a simple mixture of 2 Gaussian distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c9faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.special import psi, gamma\n",
    "\n",
    "np.random.seed(654)\n",
    "# Draw samples from two Gaussian w.p. c_i ~ Bernoulli(p)\n",
    "p = 0.75\n",
    "ps = [p, 1-p]\n",
    "mus = [2, 5]\n",
    "generative_m = np.array([stats.norm(mus[0], 1), stats.norm(mus[1], 1)])\n",
    "c_i = stats.bernoulli(p).rvs(100)\n",
    "x_i = np.array([g.rvs() for g in generative_m[c_i]])\n",
    "\n",
    "# plot generated data and the latent distributions\n",
    "x = np.linspace(-5, 12, 150)\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(x, (1-p)*generative_m[0].pdf(x))\n",
    "plt.plot(x, p*generative_m[1].pdf(x))\n",
    "plt.plot(x, (1-p)*generative_m[0].pdf(x) + p*generative_m[1].pdf(x), lw=1, ls='-.', color='black')\n",
    "plt.fill_betweenx((1-p)*generative_m[0].pdf(x), x, alpha=0.1)\n",
    "plt.fill_betweenx(p*generative_m[1].pdf(x), x, alpha=0.1)\n",
    "plt.vlines(x_i, 0, 0.01, color=np.array(['C0', 'C1'])[c_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5135d521",
   "metadata": {},
   "source": [
    "General rule: try to avoid for loop and use broadcasting as much as possible. It makes your code more efficient and less prone to errors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2edbb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some examples using broadcasting:\n",
    "z = np.random.normal(size=(2,5))\n",
    "w = np.random.normal(size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d07882",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52865893",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe4891",
   "metadata": {},
   "outputs": [],
   "source": [
    "(w[:,np.newaxis]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e516df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "(z*w[:,np.newaxis]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c4237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not do this:\n",
    "res = np.zeros(2)\n",
    "for i in range(2):\n",
    "    res[i] = np.sum(z[i,:]*w[i])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e909c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead do this:\n",
    "(z*w[:,np.newaxis]).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4835bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "(z*w[:,np.newaxis]).sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f350ee5c",
   "metadata": {},
   "source": [
    "## 3.1 EM algorithm\n",
    "\n",
    "Complete the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe66d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM:\n",
    "    def __init__(self, k):\n",
    "        self.k = k # number of clusters\n",
    "        self.n = None\n",
    "        self.mu = None # (k,)\n",
    "        self.p_ik = None # (k,n)\n",
    "        self.pi = None # (k,)\n",
    "        self.lb = [] # to keep track of the lower bound\n",
    "        self.x = None # to store the data\n",
    "\n",
    "    def expectation_step(self):\n",
    "        #\n",
    "        #your code\n",
    "        #\n",
    "        pass\n",
    "\n",
    "    def maximization_step(self):\n",
    "        #\n",
    "        # your code\n",
    "        #\n",
    "        pass\n",
    "    \n",
    "    def lower_bound(self):\n",
    "        #\n",
    "        # you code computing the lower bound\n",
    "        #\n",
    "        return -self.n/2*np.log(2*math.pi)#+your code\n",
    "\n",
    "    def fit(self, x):\n",
    "        # once other methods are implemented, this should work fine!\n",
    "        self.x = x\n",
    "        self.n = x.shape[0]\n",
    "        self.mu = np.random.uniform(x.min(), x.max(), size=self.k)\n",
    "        self.p_ik = np.zeros((self.k, self.n))\n",
    "        self.pi = np.ones(self.k) / self.k\n",
    "\n",
    "        i = -1\n",
    "        while (i<6 or ~np.isclose(self.lb[i-5], self.lb[i])):\n",
    "            self.expectation_step()\n",
    "            self.maximization_step()\n",
    "            self.lb.append(self.lower_bound())\n",
    "            i+=1\n",
    "        return self.lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c14f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "em = EM(2)\n",
    "lb = em.fit(x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16479e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the lower bound\n",
    "plt.plot(lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e30b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should display the fitted distribution\n",
    "fitted_m = [stats.norm(mu, 1) for mu in em.mu]\n",
    "fitted_p = em.pi\n",
    "\n",
    "if fitted_p[0] > fitted_p[1]:\n",
    "    fitted_p = fitted_p[::-1]\n",
    "    fitted_m = fitted_m[::-1]\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.vlines(x_i, 0, 0.01, color=np.array(['C0', 'C1'])[c_i])\n",
    "plt.plot(x, fitted_p[0]*fitted_m[0].pdf(x))\n",
    "plt.plot(x, fitted_p[1]*fitted_m[1].pdf(x))\n",
    "plt.plot(x, (1-p)*generative_m[0].pdf(x), color='black', lw=1, ls='-.')\n",
    "plt.plot(x, p*generative_m[1].pdf(x), color='black', lw=1, ls='-.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5483356",
   "metadata": {},
   "source": [
    "## 3.2 VI algorithm\n",
    "\n",
    "Complete the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d423b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these functions should be useful to compute the ELBO\n",
    "def my_beta(b):\n",
    "    return gamma(np.sum(b))/np.prod(gamma(b))\n",
    "\n",
    "def my_C(a,k):\n",
    "    return gamma(k*a)/np.float_power(gamma(a),k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d445d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VI:\n",
    "    def __init__(self,k, prec=0.0001, alpha=0.0001):\n",
    "        self.k = k\n",
    "        self.n = None \n",
    "        self.means = None # (k,)\n",
    "        self.vars = None # (k,)\n",
    "        self.phi = None # (k,n)\n",
    "        self.beta = None # (k,)\n",
    "        self.rho = None # (k,)\n",
    "        self.norm_rho = 0 # see definition in function fit\n",
    "        self.prec = prec # precision for the Gaussian prior\n",
    "        self.alpha= alpha # alpha for the Dirichlet prior\n",
    "        self.lb = [] # to keep track of the lower bound\n",
    "        self.x = None # to store the data\n",
    "        \n",
    "    def update_mu_pi(self):\n",
    "        #\n",
    "        # your code here\n",
    "        #\n",
    "        pass\n",
    "                        \n",
    "    def update_phi_rho(self):\n",
    "        #\n",
    "        # your code here\n",
    "        #\n",
    "        pass\n",
    "    \n",
    "    def lower_bound(self):\n",
    "        entrop1 = self.k/2+0.5*np.sum(np.log(2*math.pi*self.vars))\n",
    "        #\n",
    "        # your code here to compute ELBO\n",
    "        #\n",
    "        return entrop1#+your code here\n",
    "    \n",
    "    def fit(self, x):\n",
    "         # once other methods are implemented, this should work fine!\n",
    "        self.n = x.shape[0]\n",
    "        self.x = x\n",
    "        self.means = np.random.uniform(x.min(), x.max(), size=self.k)\n",
    "        self.vars = np.ones(self.k)\n",
    "        self.phi = np.random.uniform(0,1,size=((self.k,self.n)))\n",
    "        self.phi /= self.phi.sum(0)\n",
    "        self.beta = (self.n/self.k+self.alpha)*np.ones(self.k)\n",
    "        self.norm_rho = np.exp(-psi(self.k*self.alpha+self.n))\n",
    "        \n",
    "        i = -1\n",
    "        while ( i<6 or ~np.isclose(self.lb[i-5], self.lb[i])):\n",
    "            self.update_phi_rho()\n",
    "            self.update_mu_pi()\n",
    "            self.lb.append(self.lower_bound())\n",
    "            i +=1\n",
    "        return self.lb\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd2cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = VI(2)\n",
    "lb_vi = vi.fit(x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732051e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the ELBO\n",
    "plt.plot(lb_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189fafc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should display the fitted distribution\n",
    "fitted_m_vi = [stats.norm(mu, 1) for mu in vi.means]\n",
    "fitted_p_vi = vi.phi.mean(axis=1)\n",
    "\n",
    "if fitted_p_vi[0] > fitted_p_vi[1]:\n",
    "    fitted_p_vi = fitted_p_vi[::-1]\n",
    "    fitted_m_mi = fitted_m_vi[::-1]\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.vlines(x_i, 0, 0.01, color=np.array(['C0', 'C1'])[c_i])\n",
    "plt.plot(x, fitted_p_vi[0]*fitted_m_vi[0].pdf(x))\n",
    "plt.plot(x, fitted_p_vi[1]*fitted_m_vi[1].pdf(x))\n",
    "plt.plot(x, (1-p)*generative_m[0].pdf(x), color='black', lw=1, ls='-.')\n",
    "plt.plot(x, p*generative_m[1].pdf(x), color='black', lw=1, ls='-.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfa54ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-icfp",
   "language": "python",
   "name": "ml-icfp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
