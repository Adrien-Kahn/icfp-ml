{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf086f5",
   "metadata": {},
   "source": [
    "# Naive Bayes and logistic regression\n",
    "\n",
    "In this practical, we will learn about [PROBABILITY DISTRIBUTIONS - TORCH.DISTRIBUTIONS](https://pytorch.org/docs/stable/distributions.html) for a simple classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ce0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets, model_selection\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3668d78",
   "metadata": {},
   "source": [
    "## Iris dataset\n",
    "We will use the [Iris dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html). It consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. For a reference, see the following papers:\n",
    "\n",
    "- R. A. Fisher. \"The use of multiple measurements in taxonomic problems\". Annals of Eugenics. 7 (2): 179â€“188, 1936.\n",
    "\n",
    "Our goal is to construct a Naive Bayes classifier model that predicts the correct class from the sepal length and sepal width features. Under certain assumptions about this classifier model, we will explore the relation to logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63469e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b9e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iris.data[:, :2]\n",
    "targets = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3037081",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data, targets, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a787886",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0: 'Iris-Setosa', 1: 'Iris-Versicolour', 2: 'Iris-Virginica'}\n",
    "label_colours = ['blue', 'orange', 'green']\n",
    "\n",
    "def plot_data(x, y, labels, colours):\n",
    "    for c in np.unique(y):\n",
    "        inx = np.where(y == c)\n",
    "        plt.scatter(x[inx, 0], x[inx, 1], label=labels[c], c=colours[c])\n",
    "    plt.title(\"Training set\")\n",
    "    plt.xlabel(\"Sepal length (cm)\")\n",
    "    plt.ylabel(\"Sepal width (cm)\")\n",
    "    plt.legend()\n",
    "    \n",
    "plt.figure(figsize=(8, 5))\n",
    "plot_data(x_train, y_train, labels, label_colours)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec956cc",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier\n",
    "\n",
    "We will briefly review the Naive Bayes classifier model. The fundamental equation for this classifier is Bayes' rule:\n",
    "\n",
    "$$\n",
    "P(Y=y_k | X_1,\\ldots,X_d) = \\frac{P(X_1,\\ldots,X_d | Y=y_k)P(Y=y_k)}{\\sum_{k=1}^K P(X_1,\\ldots,X_d | Y=y_k)P(Y=y_k)}\n",
    "$$\n",
    "\n",
    "In the above, $d$ is the number of features or dimensions in the inputs $X$ (in our case $d=2$), and $K$ is the number of classes (in our case $K=3$). The distribution $P(Y)$ is the class prior distribution, which is a discrete distribution over $K$ classes. The distribution $P(X | Y)$ is the class-conditional distribution over inputs.\n",
    "\n",
    "The Naive Bayes classifier makes the assumption that the data features $X_i$ are conditionally independent give the class $Y$ (the 'naive' assumption). In this case, the class-conditional distribution decomposes as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(X | Y=y_k) &= P(X_1,\\ldots,X_d | Y=y_k)\\\\\n",
    "&= \\prod_{i=1}^d P(X_i | Y=y_k)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This simplifying assumption means that we typically need to estimate far fewer parameters for each of the distributions $P(X_i | Y=y_k)$ instead of the full joint distribution $P(X | Y=y_k)$.\n",
    "\n",
    "Once the class prior distribution and class-conditional densities are estimated, the Naive Bayes classifier model can then make a class prediction $\\hat{Y}$ for a new data input $\\tilde{X} := (\\tilde{X}_1,\\ldots,\\tilde{X}_d)$ according to\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{Y} &= \\text{argmax}_{y_k} P(Y=y_k | \\tilde{X}_1,\\ldots,\\tilde{X}_d) \\\\\n",
    "&= \\text{argmax}_{y_k}\\frac{P(\\tilde{X}_1,\\ldots,\\tilde{X}_d | Y=y_k)P(Y=y_k)}{\\sum_{k=1}^K P(\\tilde{X}_1,\\ldots,\\tilde{X}_d | Y=y_k)P(Y=y_k)}\\\\\n",
    "&= \\text{argmax}_{y_k} P(\\tilde{X}_1,\\ldots,\\tilde{X}_d | Y=y_k)P(Y=y_k)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66868e1",
   "metadata": {},
   "source": [
    "### Define the class prior distribution\n",
    " \n",
    "We will begin by defining the class prior distribution. To do this we will simply take the maximum likelihood estimate, given by\n",
    "\n",
    "$$\n",
    "P(Y=y_k) = \\frac{\\sum_{n=1}^N \\delta(Y^{(n)}=y_k)}{N},\n",
    "$$\n",
    "\n",
    "where the superscript $(n)$ indicates the $n$-th dataset example, $\\delta(Y^{(n)}=y_k) = 1$ if $Y^{(n)}=y_k$ and 0 otherwise, and $N$ is the total number of examples in the dataset. The above is simply the proportion of data examples belonging to class $k$.\n",
    "\n",
    "You should now write a function that builds the prior distribution from the training data, and returns it as a [`Categorical`](https://pytorch.org/docs/stable/distributions.html#categorical) Distribution object.\n",
    "\n",
    "* The input to your function `y` will be a numpy array of shape `(num_samples,)`\n",
    "* The entries in `y` will be integer labels $k=0, 1,\\ldots, K-1$\n",
    "* Your function should build and return the prior distribution as a `Categorical` distribution object\n",
    "  * The probabilities for this distribution will be a length-$K$ vector, with entries corresponding to $P(Y = y_k)$ for $k=0,1,\\ldots,K-1$\n",
    "  * Your function should work for any value of $K\\ge 1$\n",
    "  * This Distribution will have an empty batch shape and empty event shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60fc705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "def get_prior(y):\n",
    "    \"\"\"\n",
    "    This function takes training labels as a numpy array y of shape (num_samples,) as an input.\n",
    "    \"\"\"\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e826f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = get_prior(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Iris-Setosa', 'Iris-Versicolour', 'Iris-Virginica']\n",
    "plt.bar([0, 1, 2], prior.probs, color=label_colours)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Prior probability\")\n",
    "plt.title(\"Class prior distribution\")\n",
    "plt.xticks([0, 1, 2], labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010769eb",
   "metadata": {},
   "source": [
    "### Define the class-conditional densities\n",
    "\n",
    "We now turn to the definition of the class-conditional distributions $P(X_i | Y=y_k)$ for $i=0, 1$ and $k=0, 1, 2$. In our model, we will assume these distributions to be univariate Gaussian:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(X_i | Y=y_k) &= N(X_i | \\mu_{ik}, \\sigma_{ik})\\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi\\sigma_{ik}^2}} \\exp\\left\\{-\\frac{1}{2} \\left(\\frac{x - \\mu_{ik}}{\\sigma_{ik}}\\right)^2\\right\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "with mean parameters $\\mu_{ik}$ and standard deviation parameters $\\sigma_{ik}$, twelve parameters in all. We will again estimate these parameters using maximum likelihood. In this case, the estimates are given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\mu}_{ik} &= \\frac{\\sum_n X_i^{(n)} \\delta(Y^{(n)}=y_k)}{\\sum_n \\delta(Y^{(n)}=y_k)} \\\\\n",
    "\\hat{\\sigma}^2_{ik} &= \\frac{\\sum_n (X_i^{(n)} - \\hat{\\mu}_{ik})^2 \\delta(Y^{(n)}=y_k)}{\\sum_n \\delta(Y^{(n)}=y_k)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that the above are just the means and variances of the sample data points for each class.\n",
    "\n",
    "You should now write a function that computes the class-conditional Gaussian densities, using the maximum likelihood parameter estimates given above.Use [`MultivariateNormal`](https://pytorch.org/docs/stable/distributions.html#multivariatenormal) abd [`Independent`](https://pytorch.org/docs/stable/distributions.html#independent)\n",
    "\n",
    "* The inputs to the function are \n",
    "  * a numpy array `x` of shape `(num_samples, num_features)` for the data inputs\n",
    "  * a numpy array `y` of shape `(num_samples,)` for the target labels\n",
    "* Your function should work for any number of classes $K\\ge 1$ and any number of features $d\\ge 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e75cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import MultivariateNormal, Independent\n",
    "\n",
    "def get_class_conditionals(x, y):\n",
    "    \"\"\"\n",
    "    This function takes training data samples x and labels y as inputs.\n",
    "    This function should build the class-conditional Gaussian distributions above. \n",
    "    \"\"\"\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a90b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_conditionals = get_class_conditionals(x_train, y_train)\n",
    "class_conditionals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc66b54",
   "metadata": {},
   "source": [
    "We can visualise the class-conditional densities with contour plots by running the cell below. Notice how the contours of each distribution correspond to a Gaussian distribution with diagonal covariance matrix, since the model assumes that each feature is independent given the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56968b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "def get_meshgrid(x0_range, x1_range, num_points=100):\n",
    "    x0 = np.linspace(x0_range[0], x0_range[1], num_points)\n",
    "    x1 = np.linspace(x1_range[0], x1_range[1], num_points)\n",
    "    return np.meshgrid(x0, x1)\n",
    "\n",
    "def contour_plot(x0_range, x1_range, logprob_fn, batch_shape, colours, levels=None, num_points=100, dtype=dtype):\n",
    "    X0, X1 = get_meshgrid(x0_range, x1_range, num_points=num_points)\n",
    "    Z = torch.exp(logprob_fn(torch.tensor(np.expand_dims(np.array([X0.ravel(), X1.ravel()]).T, 1), dtype=dtype)))\n",
    "    Z = np.array(Z.detach()).T.reshape(batch_shape, *X0.shape)\n",
    "    for batch in np.arange(batch_shape):\n",
    "        if levels:\n",
    "            plt.contourf(X0, X1, Z[batch], alpha=0.2, colors=colours, levels=levels)\n",
    "        else:\n",
    "            plt.contour(X0, X1, Z[batch], colors=colours[batch], alpha=0.3)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_data(x_train, y_train, labels, label_colours)\n",
    "x0_min, x0_max = x_train[:, 0].min(), x_train[:, 0].max()\n",
    "x1_min, x1_max = x_train[:, 1].min(), x_train[:, 1].max()\n",
    "contour_plot((x0_min, x0_max), (x1_min, x1_max), class_conditionals.log_prob, 3, label_colours)\n",
    "plt.title(\"Training set with class-conditional density contours\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955cc88b",
   "metadata": {},
   "source": [
    "### Make predictions from the model\n",
    "\n",
    "Now the prior and class-conditional distributions are defined, you can use them to compute the model's class probability predictions for an unknown test input $\\tilde{X} = (\\tilde{X}_1,\\ldots,\\tilde{X}_d)$, according to\n",
    "\n",
    "$$\n",
    "P(Y=y_k | \\tilde{X}_1,\\ldots,\\tilde{X}_d) = \\frac{P(\\tilde{X}_1,\\ldots,\\tilde{X}_d | Y=y_k)P(Y=y_k)}{\\sum_{k=1}^K P(\\tilde{X}_1,\\ldots,\\tilde{X}_d | Y=y_k)P(Y=y_k)}\n",
    "$$\n",
    "\n",
    "The class prediction can then be taken as the class with the maximum probability:\n",
    "\n",
    "$$\n",
    "\\hat{Y} = \\text{argmax}_{y_k} P(Y=y_k | \\tilde{X}_1,\\ldots,\\tilde{X}_d)\n",
    "$$\n",
    "\n",
    "You should now write a function to return the model's class probabilities for a given batch of test inputs of shape `(batch_shape, 2)`, where the `batch_shape` has rank at least one. \n",
    "\n",
    "* The inputs to the function are the `prior` and `class_conditionals` distributions, and the inputs `x`\n",
    "* Your function should use these distributions to compute the probabilities for each class $k$ as above\n",
    "  * As before, your function should work for any number of classes $K\\ge 1$\n",
    "* It should then compute the prediction by taking the class with the highest probability\n",
    "* The predictions should be returned in a numpy array of shape `(batch_shape)`\n",
    "\n",
    "Use [`torch.logsumexp`](https://pytorch.org/docs/stable/generated/torch.logsumexp.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902d4650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(prior, class_conditionals, x):\n",
    "    \"\"\"\n",
    "    This function takes the prior distribution, class-conditional distribution, and \n",
    "    a batch of inputs in a numpy array of shape (batch_shape, 2).\n",
    "    This function should compute the class probabilities for each input in the batch, using\n",
    "    the prior and class-conditional distributions, according to the above equation.\n",
    "    \"\"\"\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_class(prior, class_conditionals, torch.tensor(x_test))\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bfc3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Test accuracy: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ebe391",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plot_data(x_train, y_train, labels, label_colours)\n",
    "x0_min, x0_max = x_train[:, 0].min(), x_train[:, 0].max()\n",
    "x1_min, x1_max = x_train[:, 1].min(), x_train[:, 1].max()\n",
    "contour_plot((x0_min, x0_max), (x1_min, x1_max), \n",
    "             lambda x: torch.log(predict_class(prior, class_conditionals, x)), \n",
    "             1, label_colours, levels=[-0.5, 0.5, 1.5, 2.5],\n",
    "             num_points=500)\n",
    "plt.title(\"Training set with decision regions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0237db",
   "metadata": {},
   "source": [
    "### Binary classifiers\n",
    "\n",
    "We will now draw a connection between the Naive Bayes classifier and logistic regression.\n",
    "\n",
    "First, we will update our model to be a binary classifier. In particular, the model will output the probability that a given input data sample belongs to the 'Iris-Setosa' class: $P(Y=y_0 | \\tilde{X}_1,\\ldots,\\tilde{X}_d)$. The remaining two classes will be pooled together with the label $y_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735bc37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_binary = np.array(y_train)\n",
    "y_train_binary[np.where(y_train_binary == 2)] = 1\n",
    "\n",
    "y_test_binary = np.array(y_test)\n",
    "y_test_binary[np.where(y_test_binary == 2)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6ad7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_binary = {0: 'Iris-Setosa', 1: 'Iris-Versicolour / Iris-Virginica'}\n",
    "label_colours_binary = ['blue', 'red']\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plot_data(x_train, y_train_binary, labels_binary, label_colours_binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc9e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_binary = get_prior(y_train_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac8db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([0, 1], prior_binary.probs.numpy(), color=label_colours_binary)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Prior probability\")\n",
    "plt.title(\"Class prior distribution\")\n",
    "plt.xticks([0, 1], labels_binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9827759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_conditionals_binary = get_class_conditionals(x_train, y_train_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plot_data(x_train, y_train_binary, labels_binary, label_colours)\n",
    "x0_min, x0_max = x_train[:, 0].min(), x_train[:, 0].max()\n",
    "x1_min, x1_max = x_train[:, 1].min(), x_train[:, 1].max()\n",
    "contour_plot((x0_min, x0_max), (x1_min, x1_max), class_conditionals_binary.log_prob, 2, label_colours)\n",
    "plt.title(\"Training set with class-conditional density contours\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3b26f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plot_data(x_train, y_train_binary, labels_binary, label_colours)\n",
    "x0_min, x0_max = x_train[:, 0].min(), x_train[:, 0].max()\n",
    "x1_min, x1_max = x_train[:, 1].min(), x_train[:, 1].max()\n",
    "contour_plot((x0_min, x0_max), (x1_min, x1_max), \n",
    "             lambda x: torch.log(predict_class(prior_binary, class_conditionals_binary, x)), \n",
    "             1, label_colours, levels=[-0.5, 0.5],\n",
    "             num_points=500)\n",
    "plt.title(\"Training set with decision regions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d595ce10",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "Adapt the code provided for the linear regression in [Module 2 -dataflowr](https://dataflowr.github.io/website/modules/2b-automatic-differentiation/) to make a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df49c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #your code here\n",
    "        \n",
    "\n",
    "epochs = 4000\n",
    "input_dim = 2 # Two inputs x1 and x2 \n",
    "output_dim = 1 \n",
    "learning_rate = 0.01\n",
    "\n",
    "model = LogisticRegression(input_dim,output_dim)\n",
    "\n",
    "criterion = #your code here\n",
    "\n",
    "optimizer = #your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ff1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,criterion,optimizer,x_train,y_train,x_test,y_test,n_epochs=epochs,freq_print=200):\n",
    "    # trainr your model\n",
    "    # print loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d74474",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plot_data(x_train, y_train_binary, labels_binary, label_colours)\n",
    "x0_min, x0_max = x_train[:, 0].min(), x_train[:, 0].max()\n",
    "x1_min, x1_max = x_train[:, 1].min(), x_train[:, 1].max()\n",
    "model.eval()\n",
    "contour_plot((x0_min, x0_max), (x1_min, x1_max), \n",
    "             lambda x: torch.log(model(x)), \n",
    "             1, ['green'], levels=[-0.5, 0.5],\n",
    "             num_points=500)\n",
    "contour_plot((x0_min, x0_max), (x1_min, x1_max), \n",
    "             lambda x: torch.log(predict_class(prior_binary, class_conditionals_binary, x)), \n",
    "             1, label_colours, levels=[-0.5, 0.5],\n",
    "             num_points=500)\n",
    "plt.title(\"Training set with decision regions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb65c6c",
   "metadata": {},
   "source": [
    "### Find the link between Naive Bayes and Logistic regression\n",
    "\n",
    "It looks like the decision boundaries are not the same for these classifiers. Find the condition you need to enforce for Naive Bayes to get the same decision boundaries (hint: have a look at the contour plots above). Check your condition by doing the math!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ac2474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-icfp",
   "language": "python",
   "name": "ml-icfp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
